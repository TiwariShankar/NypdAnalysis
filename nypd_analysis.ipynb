{"cells":[{"cell_type":"markdown","source":["## AWS S3 setup"],"metadata":{}},{"cell_type":"code","source":["ACCESS_KEY = \"\"\nSECRET_KEY = \"\".replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"nypd-motor-vehicle-collisions\"\nMOUNT_NAME = \"nypd_shankar\"\n\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["View the mount name with the %fs ls command"],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/mnt/%s\" % MOUNT_NAME))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## Exploring the NY Motor Vehicle Collisions public data with Apache Spark"],"metadata":{}},{"cell_type":"markdown","source":["View the data with the %fs ls command"],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/nypd_shankar/"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["The entry point into all functionality in Spark is the new SparkSession class:"],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Using the SparkSession, create a DataFrame from the CSV file by inferring the schema:"],"metadata":{}},{"cell_type":"code","source":["nyMotorCollisionsDF = spark.read.csv('/mnt/nypd_shankar/NYPD_Motor_Vehicle_Collisions.csv', header=True, inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Note that we are removing all space characters from the col names to prevent errors when writing to Parquet later\n\nnypdSchema = StructType([StructField('DATE', StringType(), True),\n                     StructField('TIME', StringType(), True),\n                     StructField('BOROUGH', StringType(), True),\n                     StructField('ZIPCODE', IntegerType(), True),                  \n                     StructField('LATITUDE', DoubleType(), True),       \n                     StructField('LONGITUDE', DoubleType(), True),\n                     StructField('LOCATION', StringType(), True),\n                     StructField('ON_STREET_NAME', StringType(), True),       \n                     StructField('CROSS_STREET_NAME', StringType(), True),       \n                     StructField('OFF_STREET_NAME', StringType(), True),       \n                     StructField('NUMBER_OF_PERSONS_INJURED', IntegerType(), True),       \n                     StructField('NUMBER_OF_PERSONS_KILLED', IntegerType(), True),                  \n                     StructField('NUMBER_OF_PEDESTRIANS_INJURED', IntegerType(), True),       \n                     StructField('NUMBER_OF_PEDESTRIANS_KILLED', IntegerType(), True),       \n                     StructField('NUMBER_OF_CYCLIST_INJURED', IntegerType(), True),       \n                     StructField('NUMBER_OF_CYCLIST_KILLED', IntegerType(), True),       \n                     StructField('NUMBER_OF_MOTORIST_INJURED', IntegerType(), True),       \n                     StructField('NUMBER_OF_MOTORIST_KILLED', IntegerType(), True),       \n                     StructField('CONTRIBUTING_FACTOR_VEHICLE_1', StringType(), True),                 \n                     StructField('CONTRIBUTING_FACTOR_VEHICLE_2', StringType(), True),       \n                     StructField('CONTRIBUTING_FACTOR_VEHICLE_3', StringType(), True),       \n                     StructField('CONTRIBUTING_FACTOR_VEHICLE_4', StringType(), True),       \n                     StructField('CONTRIBUTING_FACTOR_VEHICLE_5', StringType(), True),       \n                     StructField('UNIQUE_KEY', IntegerType(), True),       \n                     StructField('VEHICLE_TYPE_CODE_1', StringType(), True),       \n                     StructField('VEHICLE_TYPE_CODE_2', StringType(), True),\n                     StructField('VEHICLE_TYPE_CODE_3', StringType(), True),\n                     StructField('VEHICLE_TYPE_CODE_4', StringType(), True),\n                     StructField('VEHICLE_TYPE_CODE_5', StringType(), True)])"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#Notice that no job is run this time\nnyMotorCollisionsDF = spark.read.csv('/mnt/nypd_shankar/NYPD_Motor_Vehicle_Collisions.csv', header=True, schema=nypdSchema)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(nyMotorCollisionsDF.limit(10))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["nyMotorCollisionsDF.columns"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql.functions import *"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Let's use the unix_timestamp() function to convert the string into a timestamp:"],"metadata":{}},{"cell_type":"code","source":["from_pattern1 = 'MM/dd/yyyy'\nto_pattern1 = 'yyyy-MM-dd'\n\nnyMotorCollisionsTsDF = nyMotorCollisionsDF.withColumn('DATE_TS', unix_timestamp(nyMotorCollisionsDF['DATE'], from_pattern1).cast(\"timestamp\")) \\\n                        .drop('DATE')"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["display(nyMotorCollisionsTsDF)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Finally calculate how many distinct years of data is in the CSV file:"],"metadata":{}},{"cell_type":"code","source":["nyMotorCollisionsTsDF.select(year('DATE_TS')).distinct().orderBy('year(DATE_TS)').show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Steps for storing data frames in cache"],"metadata":{}},{"cell_type":"code","source":["nyMotorCollisionsTsDF.rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["nyMotorCollisionsTsDF.repartition(6).createOrReplaceTempView(\"nyMotorCollisionVIEW\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["spark.catalog.cacheTable(\"nyMotorCollisionVIEW\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# Call .count() to materialize the cache\nspark.table(\"nyMotorCollisionVIEW\").count()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["nyMotorCollisionDF = spark.table(\"nyMotorCollisionVIEW\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Note that the full scan + count in memory takes < 1 second!\nnyMotorCollisionDF.count()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Check if view is cached\nspark.catalog.isCached(\"nyMotorCollisionVIEW\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["%fs ls /tmp/"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Saving in parquet file format for efficient usuage \nnyMotorCollisionDF.write.format('parquet').save('/tmp/nyMotorCollisionParqt/')"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# Shows 6 gz files stored\n%fs ls /tmp/nyMotorCollisionParqt/"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# Took half second to read the file\ntempDF = spark.read.parquet('/tmp/nyMotorCollisionParqt/')"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["display(tempDF.limit(2))"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# %sql works only in databricks\n%sql SELECT count(*) FROM nyMotorCollisionVIEW;"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# Registers a DataFrame as a Temporary Table in the SQLContext. \n# To issue SQL queries via the sqlContext.sql( sqlQuery ) method.\ntempDF.registerTempTable(\"tempDF\");"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["Show dates where accidents happened:"],"metadata":{}},{"cell_type":"code","source":["data_frame = spark.sql(\"SELECT DATE_TS, COUNT(*) AS COUNT FROM tempDF GROUP BY DATE_TS ORDER BY COUNT DESC\")\nprint_frame = data_frame.toPandas();\n\nprint print_frame"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["from collections import defaultdict\nimport json\n\ndates_dict = defaultdict(list)\nfor x in range(0,24):\n    data_frame = spark.sql(\"SELECT BOROUGH,COUNT(*) AS timeCount FROM tempDF WHERE TIME LIKE '\" + str(x) + \":%' GROUP BY BOROUGH\")\n    pd_frame = data_frame.toPandas()\n    for index,row in pd_frame.iterrows():\n\t\tdates_dict[row['BOROUGH']].append(row['timeCount'])\n\ndel dates_dict[None]\nprint(json.dumps(dates_dict, indent=4))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["death_Dict = []\nfor x in range(0,24):\n\tdata_frame = spark.sql(\"SELECT COUNT(*) AS NUMBER_OF_PERSONS_KILLED FROM tempDF WHERE TIME LIKE '\" + str(x) + \":%' AND NUMBER_OF_PERSONS_KILLED > 0\")\n\tpd_frame = data_frame.toPandas()\n\tfor index,row in pd_frame.iterrows():\n\t\tdeath_Dict.append(row['NUMBER_OF_PERSONS_KILLED'])\n\nprint death_Dict"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["person_Inj_Dict = []\nfor x in range(0,24):\n\tdata_frame = spark.sql(\"SELECT COUNT(*) AS NUMBER_OF_PERSONS_INJURED FROM tempDF WHERE TIME LIKE '\" + str(x) + \":%' AND NUMBER_OF_PERSONS_INJURED > 0\")\n\tpd_frame = data_frame.toPandas()\n\tfor index,row in pd_frame.iterrows():\n\t\tperson_Inj_Dict.append(row['NUMBER_OF_PERSONS_INJURED'])\n\nprint person_Inj_Dict"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["pedes_Inj_Dict = []\nfor x in range(0,24):\n\tdata_frame = spark.sql(\"SELECT COUNT(*) AS NUMBER_OF_PEDESTRIANS_INJURED FROM tempDF WHERE TIME LIKE '\" + str(x) + \":%' AND NUMBER_OF_PEDESTRIANS_INJURED > 0\")\n\tpd_frame = data_frame.toPandas()\n\tfor index,row in pd_frame.iterrows():\n\t\tpedes_Inj_Dict.append(row['NUMBER_OF_PEDESTRIANS_INJURED'])\n\nprint pedes_Inj_Dict"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["pedes_Killed_Dict = []\nfor x in range(0,24):\n\tdata_frame = spark.sql(\"SELECT COUNT(*) AS NUMBER_OF_PEDESTRIANS_KILLED FROM tempDF WHERE TIME LIKE '\" + str(x) + \":%' AND NUMBER_OF_PEDESTRIANS_KILLED > 0\")\n\tpd_frame = data_frame.toPandas()\n\tfor index,row in pd_frame.iterrows():\n\t\tpedes_Killed_Dict.append(row['NUMBER_OF_PEDESTRIANS_KILLED'])\n\nprint pedes_Killed_Dict"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["cyclist_killed_Dict = []\nfor x in range(0,24):\n\tdata_frame = spark.sql(\"SELECT COUNT(*) AS NUMBER_OF_CYCLIST_KILLED FROM tempDF WHERE TIME LIKE '\" + str(x) + \":%' AND NUMBER_OF_CYCLIST_KILLED > 0\")\n\tpd_frame = data_frame.toPandas()\n\tfor index,row in pd_frame.iterrows():\n\t\tcyclist_killed_Dict.append(row['NUMBER_OF_CYCLIST_KILLED'])\n\nprint cyclist_killed_Dict"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["cyclist_injured_Dict = []\nfor x in range(0,24):\n\tdata_frame = spark.sql(\"SELECT COUNT(*) AS NUMBER_OF_CYCLIST_INJURED FROM tempDF WHERE TIME LIKE '\" + str(x) + \":%' AND NUMBER_OF_CYCLIST_INJURED > 0\")\n\tpd_frame = data_frame.toPandas()\n\tfor index,row in pd_frame.iterrows():\n\t\tcyclist_injured_Dict.append(row['NUMBER_OF_CYCLIST_INJURED'])\n\nprint cyclist_injured_Dict"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["motorist_injured_Dict = []\nfor x in range(0,24):\n\tdata_frame = spark.sql(\"SELECT COUNT(*) AS NUMBER_OF_MOTORIST_INJURED FROM tempDF WHERE TIME LIKE '\" + str(x) + \":%' AND NUMBER_OF_MOTORIST_INJURED > 0\")\n\tpd_frame = data_frame.toPandas()\n\tfor index,row in pd_frame.iterrows():\n\t\tmotorist_injured_Dict.append(row['NUMBER_OF_MOTORIST_INJURED'])\n\nprint motorist_injured_Dict"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["motorist_killed_Dict = []\nfor x in range(0,24):\n\tdata_frame = spark.sql(\"SELECT COUNT(*) AS NUMBER_OF_MOTORIST_KILLED FROM tempDF WHERE TIME LIKE '\" + str(x) + \":%' AND NUMBER_OF_MOTORIST_KILLED > 0\")\n\tpd_frame = data_frame.toPandas()\n\tfor index,row in pd_frame.iterrows():\n\t\tmotorist_killed_Dict.append(row['NUMBER_OF_MOTORIST_KILLED'])\n\nprint motorist_killed_Dict"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["data_frame = spark.sql(\"SELECT CONTRIBUTING_FACTOR_VEHICLE_1,COUNT(*) as count from nyMotorCollisionVIEW group By CONTRIBUTING_FACTOR_VEHICLE_1 order by count desc\")\ndf = data_frame.toPandas();\nprint df"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["data_frame = spark.sql(\"SELECT CONTRIBUTING_FACTOR_VEHICLE_2,COUNT(*) as count from nyMotorCollisionVIEW group By CONTRIBUTING_FACTOR_VEHICLE_2 order by count desc\")\ndf = data_frame.toPandas();\nprint df"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["data_frame = spark.sql(\"SELECT CONTRIBUTING_FACTOR_VEHICLE_3,COUNT(*) as count from nyMotorCollisionVIEW group By CONTRIBUTING_FACTOR_VEHICLE_3 order by count desc\")\ndf = data_frame.toPandas();\nprint df"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["data_frame = spark.sql(\"SELECT CONTRIBUTING_FACTOR_VEHICLE_4,COUNT(*) as count from nyMotorCollisionVIEW group By CONTRIBUTING_FACTOR_VEHICLE_4 order by count desc\")\ndf = data_frame.toPandas();\nprint df"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["data_frame = spark.sql(\"SELECT VEHICLE_TYPE_CODE_1,COUNT(*) as count from nyMotorCollisionVIEW group By VEHICLE_TYPE_CODE_1 order by count desc\")\ndf = data_frame.toPandas();\nprint df"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["data_frame = spark.sql(\"SELECT VEHICLE_TYPE_CODE_2,COUNT(*) as count from nyMotorCollisionVIEW group By VEHICLE_TYPE_CODE_2 order by count desc\")\ndf = data_frame.toPandas();\nprint df"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["data_frame = spark.sql(\"SELECT VEHICLE_TYPE_CODE_3,COUNT(*) as count from nyMotorCollisionVIEW group By VEHICLE_TYPE_CODE_3 order by count desc\")\ndf = data_frame.toPandas();\nprint df"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["data_frame = spark.sql(\"SELECT VEHICLE_TYPE_CODE_4,COUNT(*) as count from nyMotorCollisionVIEW group By VEHICLE_TYPE_CODE_4 order by count desc\")\ndf = data_frame.toPandas();\nprint df"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["data_frame = spark.sql(\"SELECT VEHICLE_TYPE_CODE_5,COUNT(*) as count from nyMotorCollisionVIEW group By VEHICLE_TYPE_CODE_5 order by count desc\")\ndf = data_frame.toPandas();\nprint df"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["data_frame = spark.sql(\"SELECT * from nyMotorCollisionVIEW\")\ndf = data_frame.toPandas();"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["# Locations where accident occurred most:\n\nlocation_Dict = {}\n\ndata_frame = spark.sql(\"select LATITUDE,LONGITUDE from nyMotorCollisionVIEW\")\npd_frame = data_frame.toPandas()\n\nfor index, row in pd_frame.iterrows():\n\ta = str(row['LATITUDE']) + str(row['LONGITUDE'])\n\tif a in location_Dict:\n\t\tlocation_Dict[a] = location_Dict[a] + 1;\n\telse:\n\t\tlocation_Dict[a] = 1;"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["print(json.dumps(location_Dict, indent=4))"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":61}],"metadata":{"name":"nypd_analysis","notebookId":2665752531761311},"nbformat":4,"nbformat_minor":0}
